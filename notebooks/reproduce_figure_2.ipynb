{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_cell",
   "metadata": {},
   "source": [
    "# Reproducing Figure 2G-H: oEPSC Analysis\n",
    "\n",
    "This notebook reproduces Figure 2G-H from **Zhai et al. 2025** comparing optogenetically-evoked postsynaptic currents (oEPSCs) between OffState and OnState conditions in a Parkinson's disease model.\n",
    "\n",
    "**Dataset**: DANDI:001538 - State-dependent modulation of spiny projection neurons controls levodopa-induced dyskinesia\n",
    "\n",
    "**Analysis approach**:\n",
    "- **Figure 2G**: Cumulative distribution of all individual oEPSC events\n",
    "- **Figure 2H**: Box plot comparing mean event amplitudes per experimental session\n",
    "- **Event detection**: MAD-based noise estimation with Â±5SD threshold\n",
    "- **Conditions**: OffState (control) vs OnState (L-DOPA treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "### Import Libraries and Configure Plotting Style\n",
    "\n",
    "We use the same plotting parameters as the original publication to ensure visual consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import remfile\n",
    "import seaborn as sns\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "from dotenv import load_dotenv\n",
    "from pynwb import NWBHDF5IO\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plotting style to match paper\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "def setup_figure_style():\n",
    "    \"\"\"Setup matplotlib parameters to match paper style\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 8,\n",
    "        'axes.titlesize': 10,\n",
    "        'axes.labelsize': 9,\n",
    "        'xtick.labelsize': 8,\n",
    "        'ytick.labelsize': 8,\n",
    "        'legend.fontsize': 8,\n",
    "        'figure.titlesize': 12,\n",
    "        'axes.linewidth': 0.8,\n",
    "        'axes.spines.top': False,\n",
    "        'axes.spines.right': False,\n",
    "        'xtick.major.width': 0.8,\n",
    "        'ytick.major.width': 0.8,\n",
    "        'xtick.minor.width': 0.6,\n",
    "        'ytick.minor.width': 0.6,\n",
    "    })\n",
    "\n",
    "setup_figure_style()\n",
    "print(\"Libraries imported and plotting style configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utilities_header",
   "metadata": {},
   "source": [
    "### Session ID Parsing and Filtering Functions\n",
    "\n",
    "These utility functions parse the rich metadata encoded in DANDI file paths and filter experiments by figure, measurement type, and experimental state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utilities_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_id(asset_path: str) -> str:\n",
    "    \"\"\"Extract session ID from DANDI asset path.\"\"\"\n",
    "    bottom_level_path = asset_path.split(\"/\")[1]  \n",
    "    session_id_with_ses_prefix = bottom_level_path.split(\"_\")[1]\n",
    "    session_id = session_id_with_ses_prefix.split(\"-\")[1]\n",
    "    return session_id\n",
    "\n",
    "def get_figure_number(session_id: str):\n",
    "    \"\"\"Extract which figure this data corresponds to.\"\"\"\n",
    "    return session_id.split(\"++\")[0]\n",
    "\n",
    "def get_measurement(session_id: str) -> str:\n",
    "    \"\"\"Extract measurement type.\"\"\"\n",
    "    return session_id.split(\"++\")[1]\n",
    "\n",
    "def get_state(session_id: str) -> str:\n",
    "    \"\"\"Extract experimental state.\"\"\"\n",
    "    return session_id.split(\"++\")[3]\n",
    "\n",
    "def is_figure_number(session_id: str, figure_number: str) -> bool:\n",
    "    \"\"\"Check if data belongs to a specific figure.\"\"\"\n",
    "    return get_figure_number(session_id) == figure_number\n",
    "\n",
    "def is_measurement(session_id: str, measurement: str) -> bool:\n",
    "    \"\"\"Filter data by measurement/experiment type.\"\"\"\n",
    "    return get_measurement(session_id) == measurement\n",
    "\n",
    "def is_state(session_id: str, state: str) -> bool:\n",
    "    \"\"\"Filter data by disease/treatment state.\"\"\"\n",
    "    return get_state(session_id) == state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "event_detection_header",
   "metadata": {},
   "source": [
    "### Event Detection Functions\n",
    "\n",
    "#### MAD-Based Event Detection\n",
    "\n",
    "We use **Median Absolute Deviation (MAD)** for robust noise estimation, avoiding bias from the events themselves. Events are detected as deviations >5SD from baseline and nearby events are merged to handle multi-threshold crossings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "event_detection_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nearby_events(event_times, event_amplitudes, merge_distance_ms=1.0):\n",
    "    \"\"\"Merge events within merge_distance_ms, keeping maximum amplitude.\"\"\"\n",
    "    if len(event_times) == 0:\n",
    "        return [], []\n",
    "    times = np.array(event_times)\n",
    "    amplitudes = np.array(event_amplitudes)\n",
    "    order = np.argsort(times)\n",
    "    times = times[order]\n",
    "    amplitudes = amplitudes[order]\n",
    "    merged_times = []\n",
    "    merged_amplitudes = []\n",
    "    i = 0\n",
    "    while i < len(times):\n",
    "        current_time = times[i]\n",
    "        j = i + 1\n",
    "        max_amp = amplitudes[i]\n",
    "        max_time = times[i]\n",
    "        while j < len(times) and (times[j] - current_time) <= merge_distance_ms:\n",
    "            if amplitudes[j] > max_amp:\n",
    "                max_amp = float(amplitudes[j])\n",
    "                max_time = float(times[j])\n",
    "            j += 1\n",
    "        merged_times.append(float(max_time))\n",
    "        merged_amplitudes.append(float(max_amp))\n",
    "        i = j\n",
    "    return merged_times, merged_amplitudes\n",
    "\n",
    "def process_nwb_file_for_events(asset, detection_window_shift_ms=100, event_merge_distance_ms=1.0):\n",
    "    \"\"\"Process a single NWB file and return event amplitudes using intracellular_recordings index alignment.\"\"\"\n",
    "    s3_url = asset.get_content_url(follow_redirects=1, strip_query=False)\n",
    "    file_system = remfile.File(s3_url)\n",
    "    file = h5py.File(file_system, mode=\"r\")\n",
    "    io = NWBHDF5IO(file=file, load_namespaces=True)\n",
    "    nwbfile = io.read()\n",
    "    try:\n",
    "        opto_df = nwbfile.intervals[\"optogenetic_epochs_table\"].to_dataframe()\n",
    "        det_rows = opto_df[opto_df[\"stage_name\"] == \"detection\"].sort_values(\"start_time\").reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        io.close(); file.close()\n",
    "        raise\n",
    "\n",
    "    # Iterate intracellular_recordings (ordered) and align by index to detection rows\n",
    "    ice = nwbfile.get_intracellular_recordings()\n",
    "    n_rec = len(ice.id)\n",
    "    if len(det_rows) < n_rec:\n",
    "        n_rec = len(det_rows)\n",
    "\n",
    "    file_positive_amplitudes = []\n",
    "    file_negative_amplitudes = []\n",
    "\n",
    "    for i in range(n_rec):\n",
    "        row = ice[i]\n",
    "        resp_ref = row[(\"responses\", \"response\")].iloc[0]\n",
    "        ts = resp_ref.timeseries\n",
    "        # Extract slice\n",
    "        data_A = ts.data[resp_ref.idx_start : resp_ref.idx_start + resp_ref.count]\n",
    "        t_s_all = ts.get_timestamps()\n",
    "        t_s = t_s_all[resp_ref.idx_start : resp_ref.idx_start + resp_ref.count]\n",
    "\n",
    "        data_pA = np.asarray(data_A) * 1e12\n",
    "        t_ms = np.asarray(t_s) * 1000.0  # absolute ms\n",
    "\n",
    "        det = det_rows.iloc[i]\n",
    "        det_start_ms = float(det.start_time) * 1000.0 + detection_window_shift_ms\n",
    "        det_stop_ms = float(det.stop_time) * 1000.0\n",
    "\n",
    "        m = (t_ms >= det_start_ms) & (t_ms <= det_stop_ms)\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        seg = data_pA[m]\n",
    "        seg_t = t_ms[m]\n",
    "\n",
    "        noise_median = float(np.median(seg))\n",
    "        mad = float(np.median(np.abs(seg - noise_median)))\n",
    "        mad_std = mad * 1.4826\n",
    "\n",
    "        thr_pos = noise_median + 5.0 * mad_std\n",
    "        thr_neg = noise_median - 5.0 * mad_std\n",
    "\n",
    "        idx_pos = np.where(seg > thr_pos)[0]\n",
    "        idx_neg = np.where(seg < thr_neg)[0]\n",
    "\n",
    "        t_pos_raw = seg_t[idx_pos]\n",
    "        t_neg_raw = seg_t[idx_neg]\n",
    "        a_pos_raw = seg[idx_pos] - noise_median\n",
    "        a_neg_raw = noise_median - seg[idx_neg]\n",
    "\n",
    "        pos_t, pos_a = merge_nearby_events(t_pos_raw, a_pos_raw, event_merge_distance_ms)\n",
    "        neg_t, neg_a = merge_nearby_events(t_neg_raw, a_neg_raw, event_merge_distance_ms)\n",
    "\n",
    "        file_positive_amplitudes.extend(pos_a)\n",
    "        file_negative_amplitudes.extend(neg_a)\n",
    "\n",
    "    io.close()\n",
    "    file.close()\n",
    "\n",
    "    return file_positive_amplitudes, file_negative_amplitudes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading_header",
   "metadata": {},
   "source": [
    "### Load DANDI Dataset\n",
    "\n",
    "Connect to DANDI and filter for Figure 2 optogenetic experiments, separating OffState and OnState conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "token = os.getenv(\"DANDI_API_TOKEN\")\n",
    "\n",
    "# Connect to DANDI\n",
    "dandiset_id = \"001538\"\n",
    "client = DandiAPIClient(token=token)\n",
    "client.authenticate(token=token)\n",
    "\n",
    "dandiset = client.get_dandiset(dandiset_id, \"draft\")\n",
    "assets = dandiset.get_assets()\n",
    "assets_list = list(assets)\n",
    "\n",
    "# Filter for Figure 2 oEPSC experiments\n",
    "criteria_offstate = lambda asset: (is_figure_number(get_session_id(asset.path), \"F2\") and \n",
    "                                   is_measurement(get_session_id(asset.path), \"oEPSC\") and \n",
    "                                   is_state(get_session_id(asset.path), \"OffState\"))\n",
    "\n",
    "criteria_onstate = lambda asset: (is_figure_number(get_session_id(asset.path), \"F2\") and \n",
    "                                  is_measurement(get_session_id(asset.path), \"oEPSC\") and \n",
    "                                  is_state(get_session_id(asset.path), \"OnState\"))\n",
    "\n",
    "offstate_assets = [asset for asset in assets_list if criteria_offstate(asset)]\n",
    "onstate_assets = [asset for asset in assets_list if criteria_onstate(asset)]\n",
    "\n",
    "print(f\"Found {len(offstate_assets)} OffState and {len(onstate_assets)} OnState files\")\n",
    "print(f\"Total Figure 2 oEPSC files: {len(offstate_assets) + len(onstate_assets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_processing_header",
   "metadata": {},
   "source": [
    "## Data Processing and Event Detection\n",
    "\n",
    "### Process All NWB Files\n",
    "\n",
    "We process each NWB file to extract oEPSC events, collecting both:\n",
    "- **Individual events**: For cumulative distribution analysis\n",
    "- **File means**: For box plot comparison of mean responses per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_processing_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collections\n",
    "all_offstate_events = []  # All individual events for cumulative plot\n",
    "all_onstate_events = []\n",
    "\n",
    "offstate_file_means = []  # Mean responses per file for box plot\n",
    "onstate_file_means = []\n",
    "\n",
    "# Process OffState files\n",
    "print(\"Processing OffState files...\")\n",
    "for i, asset in enumerate(tqdm(offstate_assets, desc=\"OffState files\")):\n",
    "    session_id = get_session_id(asset.path)\n",
    "    print(f\"  {i+1}/{len(offstate_assets)}: {session_id}\")\n",
    "    \n",
    "    pos_amps, neg_amps = process_nwb_file_for_events(asset)\n",
    "    all_events = pos_amps + neg_amps\n",
    "    \n",
    "    all_offstate_events.extend(all_events)\n",
    "    if len(all_events) > 0:\n",
    "        offstate_file_means.append(np.mean(all_events))\n",
    "\n",
    "# Process OnState files\n",
    "print(\"\\nProcessing OnState files...\")\n",
    "for i, asset in enumerate(tqdm(onstate_assets, desc=\"OnState files\")):\n",
    "    session_id = get_session_id(asset.path)\n",
    "    print(f\"  {i+1}/{len(onstate_assets)}: {session_id}\")\n",
    "    \n",
    "    pos_amps, neg_amps = process_nwb_file_for_events(asset)\n",
    "    all_events = pos_amps + neg_amps\n",
    "    \n",
    "    all_onstate_events.extend(all_events)\n",
    "    if len(all_events) > 0:\n",
    "        onstate_file_means.append(np.mean(all_events))\n",
    "\n",
    "print(f\"\\nData collection complete:\")\n",
    "print(f\"  OffState: {len(all_offstate_events)} events from {len(offstate_file_means)} files\")\n",
    "print(f\"  OnState: {len(all_onstate_events)} events from {len(onstate_file_means)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figure2g_header",
   "metadata": {},
   "source": [
    "## Figure 2G: Cumulative Distribution Plot\n",
    "\n",
    "### Individual Event Analysis\n",
    "\n",
    "This plot shows the cumulative distribution of **all individual oEPSC events** across all experimental sessions, allowing comparison of the full event amplitude distributions between OffState and OnState conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure2g_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cumulative distribution plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))  # Square aspect ratio like the reference\n",
    "\n",
    "# Convert to numpy arrays\n",
    "offstate_amplitudes = np.array(all_offstate_events)\n",
    "onstate_amplitudes = np.array(all_onstate_events)\n",
    "\n",
    "# Sort the data\n",
    "offstate_sorted = np.sort(offstate_amplitudes)\n",
    "onstate_sorted = np.sort(onstate_amplitudes)\n",
    "\n",
    "# Calculate cumulative probabilities as percentages\n",
    "offstate_cumulative = np.arange(1, len(offstate_sorted) + 1) / len(offstate_sorted) * 100\n",
    "onstate_cumulative = np.arange(1, len(onstate_sorted) + 1) / len(onstate_sorted) * 100\n",
    "\n",
    "# Plot with paper-style colors and thickness - match reference styling\n",
    "ax.plot(offstate_sorted, offstate_cumulative, color='black', linewidth=3)\n",
    "ax.plot(onstate_sorted, onstate_cumulative, color='gray', linewidth=3)\n",
    "\n",
    "# Formatting to match reference image\n",
    "ax.set_xlabel('oEPSC amplitude (pA)', fontsize=12, fontweight='normal')\n",
    "ax.set_ylabel('cumulative probability (%)', fontsize=12, fontweight='normal')\n",
    "\n",
    "# Set axis limits and ticks to match paper\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xticks([0, 20, 40, 60, 80])\n",
    "ax.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "\n",
    "# Style the axes to match reference\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=11, width=1.5, length=5)\n",
    "\n",
    "# Add labels with line markers in middle right\n",
    "ax.plot([55, 58], [65, 65], color='black', linewidth=3)\n",
    "ax.text(60, 65, 'off-state', fontsize=11, ha='left', va='center')\n",
    "\n",
    "ax.plot([55, 58], [55, 55], color='gray', linewidth=3)\n",
    "ax.text(60, 55, 'on-state', fontsize=11, ha='left', va='center')\n",
    "\n",
    "# Calculate and display statistics\n",
    "off_median = np.median(offstate_amplitudes)\n",
    "on_median = np.median(onstate_amplitudes)\n",
    "off_mean = np.mean(offstate_amplitudes)\n",
    "on_mean = np.mean(onstate_amplitudes)\n",
    "\n",
    "print(\"=== FIGURE 2G: CUMULATIVE DISTRIBUTION ANALYSIS ===\")\n",
    "print(f\"OffState events: {len(offstate_amplitudes)}\")\n",
    "print(f\"  Mean: {off_mean:.2f} Â± {np.std(offstate_amplitudes):.2f} pA\")\n",
    "print(f\"  Median: {off_median:.2f} pA\")\n",
    "print(f\"  25th percentile: {np.percentile(offstate_amplitudes, 25):.2f} pA\")\n",
    "print(f\"  75th percentile: {np.percentile(offstate_amplitudes, 75):.2f} pA\")\n",
    "\n",
    "print(f\"\\nOnState events: {len(onstate_amplitudes)}\")\n",
    "print(f\"  Mean: {on_mean:.2f} Â± {np.std(onstate_amplitudes):.2f} pA\")\n",
    "print(f\"  Median: {on_median:.2f} pA\")\n",
    "print(f\"  25th percentile: {np.percentile(onstate_amplitudes, 25):.2f} pA\")\n",
    "print(f\"  75th percentile: {np.percentile(onstate_amplitudes, 75):.2f} pA\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Mean fold change (OnState/OffState): {on_mean/off_mean:.3f}\")\n",
    "print(f\"  Median fold change: {on_median/off_median:.3f}\")\n",
    "\n",
    "# Kolmogorov-Smirnov test\n",
    "ks_stat, ks_p = stats.ks_2samp(offstate_amplitudes, onstate_amplitudes)\n",
    "print(f\"\\nKolmogorov-Smirnov test:\")\n",
    "print(f\"  KS statistic: {ks_stat:.4f}\")\n",
    "print(f\"  p-value: {ks_p:.2e}\")\n",
    "print(f\"  Significantly different: {'Yes' if ks_p < 0.05 else 'No'}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figure2h_header",
   "metadata": {},
   "source": [
    "## Figure 2H: Box Plot Comparison\n",
    "\n",
    "### Mean Response Per Session Analysis\n",
    "\n",
    "This box plot compares the **mean event amplitudes per experimental session**, treating each NWB file as one data point. This approach controls for potential differences in the number of events recorded per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure2h_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3.5, 4.0))\n",
    "\n",
    "# Convert to numpy arrays\n",
    "offstate_means = np.array(offstate_file_means)\n",
    "onstate_means = np.array(onstate_file_means)\n",
    "\n",
    "# Prepare data for box plot\n",
    "box_data = [offstate_means, onstate_means]\n",
    "positions = [1, 2]\n",
    "\n",
    "# Create box plot with paper-style formatting\n",
    "bp = ax.boxplot(box_data, positions=positions, patch_artist=True, \n",
    "               widths=0.4, showfliers=True, notch=False,\n",
    "               medianprops=dict(color='black', linewidth=2),\n",
    "               whiskerprops=dict(color='black', linewidth=1.5),\n",
    "               capprops=dict(color='black', linewidth=1.5),\n",
    "               flierprops=dict(marker='o', markersize=4, alpha=0.7, markerfacecolor='gray'))\n",
    "\n",
    "# Customize box colors to match paper (both white/light gray)\n",
    "colors = ['white', 'white']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor('black')\n",
    "    patch.set_linewidth(1.5)\n",
    "\n",
    "# Add individual data points as gray dots\n",
    "for i, data in enumerate(box_data):\n",
    "    # Add some jitter for visibility\n",
    "    x_vals = np.random.normal(positions[i], 0.04, size=len(data))\n",
    "    ax.scatter(x_vals, data, color='gray', s=25, alpha=0.8, zorder=3)\n",
    "\n",
    "# Calculate statistics for significance annotation\n",
    "off_mean = np.mean(offstate_means)\n",
    "on_mean = np.mean(onstate_means)\n",
    "\n",
    "# Statistical test\n",
    "u_stat, u_p = stats.mannwhitneyu(offstate_means, onstate_means, \n",
    "                                alternative='two-sided')\n",
    "\n",
    "# Add significance annotation (**) at the top\n",
    "y_max = max(np.max(offstate_means), np.max(onstate_means))\n",
    "y_sig = y_max + 0.8\n",
    "ax.text(1.5, y_sig, '**', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Formatting to match paper exactly\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['off-state', 'on-state'], fontsize=14)\n",
    "ax.set_ylabel('oEPSC amplitude (pA)', fontsize=14, fontweight='normal')\n",
    "ax.set_title('Figure 2H: dSPN oEPSC Amplitude Comparison', fontsize=16, fontweight='bold', pad=15)\n",
    "\n",
    "# Style the axes\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, width=1.5, length=5)\n",
    "ax.tick_params(axis='x', which='major', length=0)  # Remove x-axis tick marks\n",
    "\n",
    "# Calculate and display statistics\n",
    "off_median = np.median(offstate_means)\n",
    "on_median = np.median(onstate_means)\n",
    "\n",
    "print(\"=== FIGURE 2H: BOX PLOT STATISTICAL ANALYSIS ===\")\n",
    "print(f\"\\nOffState file means (n={len(offstate_means)}):\")\n",
    "print(f\"  Median: {off_median:.2f} pA\")\n",
    "print(f\"  Mean: {off_mean:.2f} Â± {np.std(offstate_means):.2f} pA\")\n",
    "print(f\"  IQR: {np.percentile(offstate_means, 25):.2f} - {np.percentile(offstate_means, 75):.2f} pA\")\n",
    "print(f\"  Range: {np.min(offstate_means):.2f} - {np.max(offstate_means):.2f} pA\")\n",
    "\n",
    "print(f\"\\nOnState file means (n={len(onstate_means)}):\")\n",
    "print(f\"  Median: {on_median:.2f} pA\")\n",
    "print(f\"  Mean: {on_mean:.2f} Â± {np.std(onstate_means):.2f} pA\") \n",
    "print(f\"  IQR: {np.percentile(onstate_means, 25):.2f} - {np.percentile(onstate_means, 75):.2f} pA\")\n",
    "print(f\"  Range: {np.min(onstate_means):.2f} - {np.max(onstate_means):.2f} pA\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Median fold change: {on_median/off_median:.3f}\")\n",
    "print(f\"  Mean fold change: {on_mean/off_mean:.3f}\")\n",
    "print(f\"  Difference in medians: {on_median - off_median:.2f} pA\")\n",
    "print(f\"  Difference in means: {on_mean - off_mean:.2f} pA\")\n",
    "\n",
    "print(f\"\\nMann-Whitney U test (non-parametric):\")\n",
    "print(f\"  U statistic: {u_stat:.2f}\")\n",
    "print(f\"  p-value: {u_p:.2e}\")\n",
    "print(f\"  Significantly different: {'Yes' if u_p < 0.05 else 'No'}\")\n",
    "\n",
    "# Welch's t-test (unequal variances)\n",
    "t_stat, t_p = stats.ttest_ind(offstate_means, onstate_means, \n",
    "                            equal_var=False)\n",
    "print(f\"\\nWelch's t-test (unequal variances):\")\n",
    "print(f\"  t statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {t_p:.2e}\")\n",
    "print(f\"  Significantly different: {'Yes' if t_p < 0.05 else 'No'}\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt(((len(offstate_means)-1)*np.var(offstate_means) + \n",
    "                    (len(onstate_means)-1)*np.var(onstate_means)) / \n",
    "                   (len(offstate_means) + len(onstate_means) - 2))\n",
    "cohens_d = (on_mean - off_mean) / pooled_std\n",
    "print(f\"\\nEffect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect_size = \"negligible\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect_size = \"small\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    effect_size = \"medium\"\n",
    "else:\n",
    "    effect_size = \"large\"\n",
    "print(f\"  Effect size interpretation: {effect_size}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "This analysis reproduces the key findings from **Figure 2G-H** of Zhai et al. 2025:\n",
    "\n",
    "1. **Cumulative Distribution (Figure 2G)**: Shows the distribution of all individual oEPSC events across experimental conditions\n",
    "2. **Box Plot Comparison (Figure 2H)**: Compares mean event amplitudes per experimental session, controlling for session-to-session variability\n",
    "\n",
    "### Methodological Notes\n",
    "\n",
    "- **Event Detection**: MAD-based noise estimation with Â±5SD threshold\n",
    "- **Artifact Avoidance**: 100ms detection window shift to avoid stimulation artifacts\n",
    "- **Event Merging**: 1ms window to handle multi-threshold crossings from single events\n",
    "- **Statistical Testing**: Both parametric and non-parametric tests for robust comparison\n",
    "\n",
    "### Biological Significance\n",
    "\n",
    "The analysis reveals how L-DOPA treatment (OnState) affects optogenetically-evoked synaptic responses in striatal neurons, providing insights into the synaptic mechanisms underlying levodopa-induced dyskinesia in Parkinson's disease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surmeier-lab-to-nwb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
