"""
Behavioral Data Interfaces for Zhai et al. 2025.

This module provides interfaces for parsing and converting behavioral data from the
Zhai et al. 2025 study, including AIM (Abnormal Involuntary Movement) scoring data.
"""

import re
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from neuroconv.basedatainterface import BaseDataInterface
from neuroconv.utils import DeepDict
from pynwb import NWBFile
from pynwb.base import TimeSeries
from pynwb.behavior import BehavioralTimeSeries
from pynwb.core import DynamicTable


class AIMBehavioralDynamicTableInterface(BaseDataInterface):
    """
    Data interface for AIM (Abnormal Involuntary Movement) behavioral assessment DynamicTable.

    This interface handles the conversion of AIM scoring data from processed CSV to NWB format,
    creating an optimized DynamicTable structure for behavioral analysis and figure reproduction.

    The AIM assessment measures dyskinesia severity using a 0-4 scale across three categories:
    - Axial dyskinesia: Abnormal posturing and twisting movements of the trunk
    - Limb dyskinesia: Abnormal movements of the forelimbs and hindlimbs
    - Orolingual dyskinesia: Abnormal jaw and tongue movements

    Parameters
    ----------
    processed_data_csv_path : Path
        Path to the pre-processed CSV file containing all AIM scoring data (cached from Excel processing)
    session_date : str
        Session date in YYYY-MM-DD format to filter data for this specific session
    session_number : int
        Session number within the day (1, 2, 3, 4, 5) to filter data
    animal_id : int
        Animal identifier to filter data for this specific animal
    verbose : bool, default=False
        Enable verbose output during data processing

    Notes
    -----
    This interface reads from a pre-processed CSV file that contains data already transformed
    through the comprehensive pipeline. The CSV is generated by the main script and cached
    for efficient repeated access without re-processing the Excel data.

    The resulting NWB DynamicTable enables trivial Figure 7J reproduction with
    optimized data access patterns for behavioral analysis.
    """

    def __init__(
        self,
        processed_data_csv_path: Path,
        session_date: str,
        session_number: int,
        animal_id: int,
        verbose: bool = False,
    ):
        """
        Initialize the AIM behavioral DynamicTable interface.

        Parameters
        ----------
        processed_data_csv_path : Path
            Path to the pre-processed CSV file containing all AIM scoring data
        session_date : str
            Session date in YYYY-MM-DD format
        session_number : int
            Session number within the day (1-5)
        animal_id : int
            Animal identifier
        verbose : bool, default=False
            Enable verbose output
        """
        self.processed_data_csv_path = Path(processed_data_csv_path)
        self.session_date = session_date
        self.session_number = session_number
        self.animal_id = animal_id
        self.verbose = verbose

        # Validate file exists
        if not self.processed_data_csv_path.exists():
            raise FileNotFoundError(f"Processed data CSV file not found: {self.processed_data_csv_path}")

        # Load and filter the data for this session/animal
        self._load_session_data()

    def _load_session_data(self):
        """
        Load the pre-processed AIM data and filter for the specific session and animal.
        """
        if self.verbose:
            print(f"Loading AIM data for animal {self.animal_id}, session {self.session_date} #{self.session_number}")

        # Load the complete pre-processed dataset from CSV
        complete_data = pd.read_csv(self.processed_data_csv_path)

        # Filter for this specific session and animal
        session_mask = (
            (complete_data["session_date"] == self.session_date)
            & (complete_data["session_number"] == self.session_number)
            & (complete_data["animal_id"] == self.animal_id)
        )

        self.session_data = complete_data[session_mask].copy()

        if len(self.session_data) == 0:
            raise ValueError(
                f"No data found for animal {self.animal_id} on {self.session_date} session {self.session_number}"
            )

        # Extract metadata for this animal/session
        self.genotype = self.session_data["genotype"].iloc[0]
        self.time_points = sorted(self.session_data["time_minutes"].unique())

        if self.verbose:
            print(f"  Found {len(self.session_data)} time points: {self.time_points}")
            print(f"  Animal genotype: {self.genotype}")

    def get_metadata(self) -> DeepDict:
        """
        Get metadata for the AIM behavioral assessment.

        Returns
        -------
        DeepDict
            Metadata dictionary containing behavioral experiment information
        """
        metadata = DeepDict()

        # Add behavioral metadata
        metadata["Behavior"] = {
            "name": "AIMBehavioralAssessment",
            "description": "Abnormal Involuntary Movement (AIM) scoring for dyskinesia assessment",
            "session_date": self.session_date,
            "session_number": self.session_number,
            "animal_id": self.animal_id,
            "genotype": self.genotype,
            "time_points_minutes": self.time_points,
            "scoring_scale": "0-4 scale where 0=no abnormal movements, 4=continuous without interruptions",
            "score_categories": ["axial", "limb", "orolingual"],
            "measurement_protocol": "L-DOPA induced dyskinesia assessment at specified time intervals",
        }

        return metadata

    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[dict] = None):
        """
        Add AIM behavioral DynamicTable to the NWB file.

        Parameters
        ----------
        nwbfile : NWBFile
            NWB file to add the behavioral data to
        metadata : dict | None
            Optional metadata dictionary
        """
        metadata = metadata or self.get_metadata()

        # Create or get behavioral processing module
        if "behavior" not in nwbfile.processing:
            behavioral_module = nwbfile.create_processing_module(
                name="behavior",
                description="Behavioral data including AIM scoring optimized for Figure 7J reproduction",
            )
        else:
            behavioral_module = nwbfile.processing["behavior"]

        # Create optimized AIM score table directly
        aim_table = DynamicTable(
            name="DynamicTableAIMScore",
            description="Abnormal Involuntary Movement scores optimized for Figure 7J reproduction. "
            "Each row represents one time point for one animal. Scores range 0-4 where "
            "0=no abnormal movements, 4=continuous without interruptions.",
        )

        # Add columns to the table first (empty)
        aim_table.add_column(name="animal_id", description="Animal identifier")
        aim_table.add_column(name="genotype", description="Animal genotype (CDGI Knockout/Wild Type)")
        aim_table.add_column(name="session_date", description="Session date (YYYY-MM-DD)")
        aim_table.add_column(name="session_number", description="Session number within day (1-5)")
        aim_table.add_column(name="time_minutes", description="Time post L-DOPA (minutes)")
        aim_table.add_column(name="timestamps", description="Time post L-DOPA (seconds)")
        aim_table.add_column(name="axial_score", description="Axial dyskinesia score (0-4)")
        aim_table.add_column(name="limb_score", description="Limb dyskinesia score (0-4)")
        aim_table.add_column(name="orolingual_score", description="Orolingual dyskinesia score (0-4)")
        aim_table.add_column(name="total_score", description="Total AIM score (sum of components)")

        # Add rows
        for _, row in self.session_data.iterrows():
            aim_table.add_row(
                animal_id=int(row["animal_id"]),
                genotype=row["genotype"],
                session_date=row["session_date"],
                session_number=int(row["session_number"]),
                time_minutes=int(row["time_minutes"]),
                timestamps=float(row["timestamps"]),
                axial_score=float(row["axial"]) if pd.notna(row["axial"]) else np.nan,
                limb_score=float(row["limb"]) if pd.notna(row["limb"]) else np.nan,
                orolingual_score=float(row["orolingual"]) if pd.notna(row["orolingual"]) else np.nan,
                total_score=float(row["total_score"]) if pd.notna(row["total_score"]) else np.nan,
            )

        behavioral_module.add(aim_table)


class AIMBehavioralTimeSeriesInterface(BaseDataInterface):
    """
    Data interface for AIM (Abnormal Involuntary Movement) behavioral assessment as BehavioralTimeSeries.

    This interface handles the conversion of AIM scoring data from processed CSV to NWB format,
    creating BehavioralTimeSeries objects for time-aligned analysis and correlation with other signals.

    The AIM assessment measures dyskinesia severity using a 0-4 scale across three categories:
    - Axial dyskinesia: Abnormal posturing and twisting movements of the trunk
    - Limb dyskinesia: Abnormal movements of the forelimbs and hindlimbs
    - Orolingual dyskinesia: Abnormal jaw and tongue movements

    Creates one BehavioralTimeSeries container with four TimeSeries components:
    - "aim_axial": Axial component scores
    - "aim_limb": Limb component scores
    - "aim_orolingual": Orolingual component scores
    - "aim_total": Sum of all three components

    Parameters
    ----------
    processed_data_csv_path : Path
        Path to the pre-processed CSV file containing all AIM scoring data (cached from Excel processing)
    session_date : str
        Session date in YYYY-MM-DD format to filter data for this specific session
    session_number : int
        Session number within the day (1, 2, 3, 4, 5) to filter data
    animal_id : int
        Animal identifier to filter data for this specific animal
    verbose : bool, default=False
        Enable verbose output during data processing

    Notes
    -----
    This interface creates a single BehavioralTimeSeries container with multiple TimeSeries:
    - One container holds all four AIM scoring components
    - Timestamps are bin midpoints in seconds: [1200, 2400, 3600, 4800, 6000, 7200]
    - Data are float64 vectors of scores (0-4) for each component
    - Missing bins are omitted from timestamps (no NaN filling)
    - Enables quick time-aligned plots and correlation with other continuous signals
    """

    def __init__(
        self,
        processed_data_csv_path: Path,
        session_date: str,
        session_number: int,
        animal_id: int,
        verbose: bool = False,
    ):
        """
        Initialize the AIM behavioral TimeSeries interface.

        Parameters
        ----------
        processed_data_csv_path : Path
            Path to the pre-processed CSV file containing all AIM scoring data
        session_date : str
            Session date in YYYY-MM-DD format
        session_number : int
            Session number within the day (1-5)
        animal_id : int
            Animal identifier
        verbose : bool, default=False
            Enable verbose output
        """
        self.processed_data_csv_path = Path(processed_data_csv_path)
        self.session_date = session_date
        self.session_number = session_number
        self.animal_id = animal_id
        self.verbose = verbose

        # Validate file exists
        if not self.processed_data_csv_path.exists():
            raise FileNotFoundError(f"Processed data CSV file not found: {self.processed_data_csv_path}")

        # Load and filter the data for this session/animal
        self._load_session_data()

    def _load_session_data(self):
        """
        Load the pre-processed AIM data and filter for the specific session and animal.
        """
        if self.verbose:
            print(
                f"Loading AIM TimeSeries data for animal {self.animal_id}, session {self.session_date} #{self.session_number}"
            )

        # Load the complete pre-processed dataset from CSV
        complete_data = pd.read_csv(self.processed_data_csv_path)

        # Filter for this specific session and animal
        session_mask = (
            (complete_data["session_date"] == self.session_date)
            & (complete_data["session_number"] == self.session_number)
            & (complete_data["animal_id"] == self.animal_id)
        )

        self.session_data = complete_data[session_mask].copy()

        if len(self.session_data) == 0:
            raise ValueError(
                f"No data found for animal {self.animal_id} on {self.session_date} session {self.session_number}"
            )

        # Extract metadata for this animal/session
        self.genotype = self.session_data["genotype"].iloc[0]
        self.time_points = sorted(self.session_data["time_minutes"].unique())

        # Convert time points to timestamps (bin midpoints in seconds)
        self.timestamps = np.array([t * 60.0 for t in self.time_points], dtype=np.float64)

        if self.verbose:
            print(f"  Found {len(self.session_data)} time points: {self.time_points}")
            print(f"  Timestamps (seconds): {self.timestamps}")

    def get_metadata(self) -> DeepDict:
        """
        Extract metadata for the BehavioralTimeSeries objects.

        Returns
        -------
        DeepDict
            Dictionary containing metadata
        """
        metadata = super().get_metadata()

        # Behavioral processing module metadata
        metadata["Behavior"] = {
            "BehavioralTimeSeries": {
                "aim_axial": {
                    "name": "TimeSeriesAIMAxial",
                    "description": "AIM scoring for axial dyskinesia (abnormal posturing and twisting movements of the trunk)",
                    "unit": "score",
                    "comments": "Scores range from 0-4, with higher scores indicating more severe dyskinesia",
                },
                "aim_limb": {
                    "name": "TimeSeriesAIMLimb",
                    "description": "AIM scoring for limb dyskinesia (abnormal movements of the forelimbs and hindlimbs)",
                    "unit": "score",
                    "comments": "Scores range from 0-4, with higher scores indicating more severe dyskinesia",
                },
                "aim_orolingual": {
                    "name": "TimeSeriesAIMOrolingual",
                    "description": "AIM scoring for orolingual dyskinesia (abnormal jaw and tongue movements)",
                    "unit": "score",
                    "comments": "Scores range from 0-4, with higher scores indicating more severe dyskinesia",
                },
                "aim_total": {
                    "name": "TimeSeriesAIMTotal",
                    "description": "Total AIM score (sum of axial, limb, and orolingual components)",
                    "unit": "score",
                    "comments": "Total scores range from 0-12, representing combined dyskinesia severity",
                },
            }
        }

        return metadata

    def add_to_nwbfile(self, nwbfile: NWBFile, metadata: Optional[Dict] = None):
        """
        Add the AIM BehavioralTimeSeries data to the NWB file.

        Parameters
        ----------
        nwbfile : NWBFile
            NWB file to add the data to
        metadata : dict | None
            Metadata dictionary containing BehavioralTimeSeries parameters
        """
        # Get organized metadata
        metadata = metadata or self.get_metadata()

        # Create behavioral processing module if it doesn't exist
        if "behavior" not in nwbfile.processing:
            behavioral_module = nwbfile.create_processing_module(
                name="behavior",
                description="Behavioral data including AIM scoring for dyskinesia assessment",
            )
        else:
            behavioral_module = nwbfile.processing["behavior"]

        # Sort session data by time for consistent ordering
        session_data_sorted = self.session_data.sort_values("time_minutes")

        # Extract data arrays for each component
        axial_data = session_data_sorted["axial"].values.astype(np.float32)
        limb_data = session_data_sorted["limb"].values.astype(np.float32)
        orolingual_data = session_data_sorted["orolingual"].values.astype(np.float32)
        total_data = session_data_sorted["total_score"].values.astype(np.float32)

        # Create BehavioralTimeSeries objects for each component
        components = [
            ("aim_axial", axial_data),
            ("aim_limb", limb_data),
            ("aim_orolingual", orolingual_data),
            ("aim_total", total_data),
        ]

        # Check if BehavioralTimeSeries already exists
        behavioral_timeseries_name = "BehavioralTimeSeriesAIM"
        if behavioral_timeseries_name in behavioral_module.data_interfaces:
            if self.verbose:
                print(f"  Skipping {behavioral_timeseries_name} - already exists in behavioral module")
            return

        # Create individual TimeSeries for each component
        time_series_list = []
        for component_name, component_data in components:
            component_metadata = metadata["Behavior"]["BehavioralTimeSeries"][component_name]

            timeseries = TimeSeries(
                name=component_metadata["name"],
                description=component_metadata["description"],
                data=component_data,
                timestamps=self.timestamps,
                unit=component_metadata["unit"],
                comments=component_metadata["comments"],
            )

            time_series_list.append(timeseries)

            if self.verbose:
                print(f"  Created TimeSeries: {component_name}")
                print(f"    Data shape: {component_data.shape}")
                print(f"    Timestamps shape: {self.timestamps.shape}")
                print(f"    Data range: {component_data.min():.1f} - {component_data.max():.1f}")

        # Create single BehavioralTimeSeries container with all TimeSeries
        behavioral_timeseries = BehavioralTimeSeries(name=behavioral_timeseries_name, time_series=time_series_list)

        behavioral_module.add(behavioral_timeseries)

        if self.verbose:
            print(f"  Added BehavioralTimeSeries container: {behavioral_timeseries_name}")
            print(f"    Contains {len(time_series_list)} TimeSeries components")
            print(f"    Components: {[ts.name for ts in time_series_list]}")


def build_source_data_from_aim_excel_table(
    excel_path: Path, genotype_csv_path: Path, verbose: bool = False
) -> pd.DataFrame:
    """
    Build source data from AIM Excel table through complete processing pipeline.

    This function unifies the entire data processing workflow:
    1. Parse Excel to tidy wide format
    2. Validate against test suite (23 hard-coded test cases)
    3. Transform to long format
    4. Pivot to final structure optimized for NWB DynamicTable

    The result is a DataFrame perfectly structured for NWB DynamicTable creation
    and trivial Figure 7J reproduction.

    Parameters
    ----------
    excel_path : Path
        Path to AIM Excel file
    genotype_csv_path : Path
        Path to genotype mapping CSV file
    verbose : bool, default=False
        Enable verbose output showing each processing step

    Returns
    -------
    pd.DataFrame
        Final pivot DataFrame ready for NWB DynamicTable creation with columns:
        - animal_id: Animal identifier
        - session_date: Session date in YYYY-MM-DD format
        - session_number: Session number within the day (1, 2, 3, 4, 5)
        - genotype: Animal genotype (CDGI Knockout, Wild Type, or unknown)
        - time_minutes: Time post L-DOPA in minutes
        - timestamps: Time in seconds (for NWB timestamps)
        - axial: Axial dyskinesia score (0-4)
        - limb: Limb dyskinesia score (0-4)
        - orolingual: Orolingual dyskinesia score (0-4)
        - total_score: Total AIM score (sum of components)

    Raises
    ------
    ValueError
        If validation tests fail, indicating parsing errors


    """
    if verbose:
        print("Building source data from AIM Excel table...")
        print("=" * 60)

    # Step 1: Parse Excel to tidy format
    wide_df = _parse_aim_excel_to_tidy_format(excel_path, genotype_csv_path, verbose)

    # Step 2: Validate against test suite
    test_passed, test_results = _validate_tidy_data(wide_df, verbose)
    if not test_passed:
        raise ValueError(f"Validation failed: {test_results['failed']}/{test_results['total']} tests failed")

    # Step 3: Transform to long format
    long_df = _transform_tidy_to_long_format(wide_df, verbose)

    # Step 4: Pivot to final structure
    final_df = _pivot_long_to_final_structure(long_df, verbose)

    if verbose:
        print("=" * 60)
        print("Pipeline completed successfully!")
        print(f"Final dataset: {len(final_df)} observations from {final_df['animal_id'].nunique()} animals")
        print(f"Ready for NWB DynamicTable creation and Figure 7J reproduction")

    return final_df


# Hard-coded test cases extracted directly from Excel sheets
EXPECTED_VALUES = [
    # Sheet 11202017, Animal 4124, Session 2017-11-20
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-20",
        "animal_id": 4124,
        "score_type": "axial",
        "expected": {"20min": 3.0, "40min": 3.0, "60min": 4.0, "80min": 4.0, "100min": 3.0},
    },
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-20",
        "animal_id": 4124,
        "score_type": "limb",
        "expected": {"20min": 1.5, "40min": 2.5, "60min": 2.0, "80min": 2.0, "100min": 1.0},
    },
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-20",
        "animal_id": 4124,
        "score_type": "orolingual",
        "expected": {"20min": 2.0, "40min": 2.0, "60min": 2.0, "80min": 1.5, "100min": 3.0},
    },
    # Sheet 11202017, Animal 4124, Session 2017-11-22
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-22",
        "animal_id": 4124,
        "score_type": "axial",
        "expected": {"20min": 4.0, "40min": 3.0, "60min": 3.0, "80min": 3.5, "100min": 4.0, "120min": 3.0},
    },
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-22",
        "animal_id": 4124,
        "score_type": "orolingual",
        "expected": {"20min": 1.0, "40min": 1.0, "60min": 2.0, "80min": 2.0, "100min": 2.0, "120min": 1.5},
    },
    # Sheet 02232018, Animal 1590, Session 2018-02-23
    {
        "sheet_name": 2232018,
        "session_date": "2018-02-23",
        "animal_id": 1590,
        "score_type": "axial",
        "expected": {"20min": 4.0, "40min": 4.0, "60min": 4.0, "80min": 4.0},
    },
    {
        "sheet_name": 2232018,
        "session_date": "2018-02-23",
        "animal_id": 1590,
        "score_type": "limb",
        "expected": {"20min": 3.0, "40min": 3.0, "60min": 3.5, "80min": 3.5},
    },
    # Sheet 02232018, Animal 1944, Session 2018-03-04
    {
        "sheet_name": 2232018,
        "session_date": "2018-03-04",
        "animal_id": 1944,
        "score_type": "limb",
        "expected": {"20min": 2.0, "40min": 1.5, "60min": 2.0, "80min": 2.0},
    },
    # Sheet 11042020, Animal 2519, Multiple sessions (this was the problematic one)
    {
        "sheet_name": 11042020,
        "session_date": "2020-11-04",
        "animal_id": 2519,
        "score_type": "orolingual",
        "expected": {"20min": 2.0, "40min": 2.0, "60min": 2.0, "80min": 2.0},
    },
    {
        "sheet_name": 11042020,
        "session_date": "2020-11-06",
        "animal_id": 2519,
        "score_type": "orolingual",
        "expected": {"20min": 2.0, "40min": 2.5, "60min": 2.0, "80min": 2.0},
    },
    {
        "sheet_name": 11042020,
        "session_date": "2020-11-09",
        "animal_id": 2519,
        "score_type": "orolingual",
        "expected": {"20min": 2.0, "40min": 2.5, "60min": 2.0, "80min": 2.0, "100min": 1.5},
    },
    {
        "sheet_name": 11042020,
        "session_date": "2020-11-11",
        "animal_id": 2519,
        "score_type": "orolingual",
        "expected": {"20min": 2.5, "40min": 2.0, "60min": 2.0, "80min": 2.0, "100min": 1.5, "120min": 1.0},
    },
    # Sheet 03202018, Animal 2587, Session 2018-03-20
    {
        "sheet_name": 3202018,
        "session_date": "2018-03-20",
        "animal_id": 2587,
        "score_type": "axial",
        "expected": {"20min": 4.0, "40min": 4.0, "60min": 4.0, "80min": 4.0},
    },
    {
        "sheet_name": 3202018,
        "session_date": "2018-03-20",
        "animal_id": 2587,
        "score_type": "orolingual",
        "expected": {"20min": 3.0, "40min": 2.5, "60min": 2.5, "80min": 2.5},
    },
    # Sheet 09052018, Animal 6571, Session 2018-09-05
    {
        "sheet_name": 9052018,
        "session_date": "2018-09-05",
        "animal_id": 6571,
        "score_type": "axial",
        "expected": {"20min": 3.5, "40min": 4.0, "60min": 4.0, "80min": 4.0},
    },
    # Sheet 04012018, Animal 2828, Session 2018-04-01
    {
        "sheet_name": 4012018,
        "session_date": "2018-04-01",
        "animal_id": 2828,
        "score_type": "limb",
        "expected": {"20min": 1.0, "40min": 2.0, "60min": 2.0, "80min": 2.0},
    },
    # Additional random test cases for more comprehensive testing
    # Sheet 02232018, Animal 1945, Session 2018-02-25
    {
        "sheet_name": 2232018,
        "session_date": "2018-02-25",
        "animal_id": 1945,
        "score_type": "axial",
        "expected": {"20min": 4.0, "40min": 4.0, "60min": 4.0, "80min": 4.0},
    },
    # Sheet 11202017, Animal 4123, Session 2017-11-24
    {
        "sheet_name": 11202017,
        "session_date": "2017-11-24",
        "animal_id": 4123,
        "score_type": "orolingual",
        "expected": {"20min": 3.0, "40min": 3.0, "60min": 3.0, "80min": 3.0, "100min": 2.0},
    },
    # Sheet 09052018, Animal 5940, Session 2018-09-08
    {
        "sheet_name": 9052018,
        "session_date": "2018-09-08",
        "animal_id": 5940,
        "score_type": "limb",
        "expected": {"20min": 2.5, "40min": 3.5, "60min": 2.5, "80min": 3.0, "100min": 3.5},
    },
    # Sheet 04012018, Animal 2636, Session 2018-04-03
    {
        "sheet_name": 4012018,
        "session_date": "2018-04-03",
        "animal_id": 2636,
        "score_type": "axial",
        "expected": {"20min": 4.0, "40min": 4.0, "60min": 4.0, "80min": 4.0, "100min": 4.0},
    },
    # Sheet 7182018, Animal 5871, Session 2018-07-20
    {
        "sheet_name": 7182018,
        "session_date": "2018-07-20",
        "animal_id": 5871,
        "score_type": "orolingual",
        "expected": {"20min": 4.0, "40min": 3.0, "60min": 2.0, "80min": 2.5, "100min": 2.0},
    },
    # Sheet 11042020, Animal 2516, Session 2020-11-09
    {
        "sheet_name": 11042020,
        "session_date": "2020-11-09",
        "animal_id": 2516,
        "score_type": "limb",
        "expected": {"20min": 2.0, "40min": 2.0, "60min": 2.5, "80min": 2.0, "100min": 2.5},
    },
    # Sheet 03202018, Animal 2586, Session 2018-03-25
    {
        "sheet_name": 3202018,
        "session_date": "2018-03-25",
        "animal_id": 2586,
        "score_type": "orolingual",
        "expected": {"20min": 2.0, "40min": 2.0, "60min": 2.5, "80min": 3.0, "100min": 2.5},
    },
]


def parse_date_from_cell(cell_val) -> Optional[datetime]:
    """Parse various date formats from Excel cells."""
    if pd.isna(cell_val):
        return None

    if isinstance(cell_val, datetime):
        return cell_val

    cell_str = str(cell_val).strip()

    # Pattern 1: Date:MMDDYYYY format
    match = re.search(r"Date:\s*(\d{8})", cell_str)
    if match:
        date_str = match.group(1)
        month = date_str[:2]
        day = date_str[2:4]
        year = date_str[4:8]
        try:
            return datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
        except:
            pass

    # Pattern 2: M/D/YYYY format
    match = re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", cell_str)
    if match:
        try:
            return datetime.strptime(match.group(0), "%m/%d/%Y")
        except:
            pass

    return None


def find_session_boundaries(df: pd.DataFrame) -> List[Tuple[int, datetime]]:
    """Find all session start rows and their dates in the dataframe."""
    sessions = []

    for i in range(len(df)):
        date = parse_date_from_cell(df.iloc[i, 0])
        if date:
            sessions.append((i, date))

    return sessions


def parse_aim_excel_to_wide_format(
    excel_path: Path, genotype_csv_path: Optional[Path] = None, output_path: Optional[Path] = None
) -> pd.DataFrame:
    """
    Parse AIM scoring data from Excel file with correct session date handling.

    Parameters
    ----------
    excel_path : Path
        Path to the AIM testing Excel file
    genotype_csv_path : Path, optional
        Path to the genotype CSV file for automatic genotype assignment
    output_path : Path, optional
        Path to save the tidy CSV file. If None, CSV is not saved.

    Returns
    -------
    pd.DataFrame
        Tidy DataFrame with columns:
        - sheet_name: Original sheet name as integer
        - session_date: Session date in YYYY-MM-DD format
        - session_number: Session number within the day (1, 2, 3, 4, 5)
        - animal_id: Animal identifier as integer
        - genotype: Animal genotype (CDGI Knockout, Wild Type, or unknown)
        - score_type: Type of AIM score (axial, limb, orolingual)
        - 20min, 40min, 60min, 80min, 100min, 120min: Score values at each time point

    Notes
    -----
    This function correctly handles:
    - Multiple sessions per sheet with different dates
    - Proper association of animals with their scoring data
    - Genotype assignment from external CSV file
    - Various date formats in Excel sheets
    """
    excel_file = pd.ExcelFile(excel_path)
    all_data = []

    # Load genotype data if provided
    genotype_map = {}
    if genotype_csv_path and genotype_csv_path.exists():
        genotype_df = pd.read_csv(genotype_csv_path, skiprows=4)  # Skip first 4 rows
        for _, row in genotype_df.iterrows():
            if pd.notna(row["MOUSE"]) and pd.notna(row["Category"]):
                animal_id = str(row["MOUSE"]).replace(",", "")  # Remove commas
                category = row["Category"]
                if category == "ko":
                    genotype_map[animal_id] = "CDGI Knockout"
                elif category in ["CONT", "cont"]:
                    genotype_map[animal_id] = "Wild Type"

    for sheet_name in excel_file.sheet_names:
        # Skip non-date sheets
        if not sheet_name.isdigit() or len(sheet_name) != 8:
            continue

        df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)

        # Find all session boundaries
        sessions = find_session_boundaries(df)

        if not sessions:
            # No clear date markers, use sheet name as fallback
            month = sheet_name[:2]
            day = sheet_name[2:4]
            year = sheet_name[4:8]
            default_date = datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
            sessions = [(0, default_date)]

        # Process each session
        for session_idx, (start_row, session_date) in enumerate(sessions):
            # Session number starts from 1
            session_number = session_idx + 1

            # Determine end row for this session
            if session_idx < len(sessions) - 1:
                end_row = sessions[session_idx + 1][0]
            else:
                end_row = len(df)

            # For the first session, start from the beginning of the sheet
            # For subsequent sessions, start after the date marker
            if session_idx == 0:
                search_start = 0
            else:
                search_start = start_row + 1

            # Find all animals in this session
            i = search_start
            while i < end_row:
                row = df.iloc[i]

                # Look for animal ID (ET# pattern)
                if pd.notna(row.iloc[0]) and "ET#" in str(row.iloc[0]):
                    animal_id_str = str(row.iloc[0])

                    # Extract genotype from cell if present
                    genotype = "unknown"
                    if "(KO)" in animal_id_str:
                        genotype = "CDGI Knockout"
                    elif "(WT)" in animal_id_str:
                        genotype = "Wild Type"

                    # Clean animal ID
                    animal_id = animal_id_str.replace("(KO)", "").replace("(WT)", "").replace("ET#", "").strip()

                    # Override with genotype map if available
                    if animal_id in genotype_map:
                        genotype = genotype_map[animal_id]

                    # Find scoring rows - look for the pattern where animal ID and Ax are in same row
                    ax_row = None
                    li_row = None
                    ol_row = None

                    # Check if Ax is in the same row as the animal ID
                    if len(df.iloc[i]) > 1 and "Ax" in str(df.iloc[i].iloc[1]):
                        ax_row = i
                        # Li should be in next row, Ol in row after that
                        if i + 1 < end_row and len(df.iloc[i + 1]) > 1 and "Li" in str(df.iloc[i + 1].iloc[1]):
                            li_row = i + 1
                        if i + 2 < end_row and len(df.iloc[i + 2]) > 1 and "Ol" in str(df.iloc[i + 2].iloc[1]):
                            ol_row = i + 2

                    # Extract scores for each category
                    time_points = ["20min", "40min", "60min", "80min", "100min", "120min"]

                    # Process each score type
                    for score_row, score_type in [(ax_row, "axial"), (li_row, "limb"), (ol_row, "orolingual")]:
                        if score_row is not None:
                            scores = {}

                            for col_idx, time_pt in enumerate(time_points, start=2):
                                if col_idx < len(df.columns):
                                    val = df.iloc[score_row, col_idx]
                                    if pd.notna(val) and str(val).strip() != "":
                                        try:
                                            scores[time_pt] = float(val)
                                        except (ValueError, TypeError):
                                            scores[time_pt] = np.nan
                                    else:
                                        scores[time_pt] = np.nan
                                else:
                                    scores[time_pt] = np.nan

                            # Add row to data
                            row_data = {
                                "sheet_name": int(sheet_name),
                                "session_date": session_date.strftime("%Y-%m-%d"),
                                "session_number": session_number,
                                "animal_id": int(animal_id),
                                "genotype": genotype,
                                "score_type": score_type,
                            }
                            row_data.update(scores)
                            all_data.append(row_data)

                    i = max(ax_row or i, li_row or i, ol_row or i) + 1
                else:
                    i += 1

    # Create DataFrame
    wide_df = pd.DataFrame(all_data)

    # Sort by sheet, date, session_number, animal, and score type
    if not wide_df.empty:
        wide_df = wide_df.sort_values(["sheet_name", "session_date", "session_number", "animal_id", "score_type"])
        wide_df = wide_df.reset_index(drop=True)

    # Save to CSV if output path provided
    if output_path:
        wide_df.to_csv(output_path, index=False)
        print(f"AIM data saved to: {output_path}")

    return wide_df


def test_aim_data_accuracy(df: pd.DataFrame, verbose: bool = False) -> Dict[str, Any]:
    """
    Test the tidy data against hard-coded expected values.

    Parameters
    ----------
    df : pd.DataFrame
        Tidy DataFrame with AIM data to test
    verbose : bool, default=False
        Enable verbose output showing individual test results

    Returns
    -------
    Dict[str, Any]
        Test results with pass/fail status and details
    """
    if verbose:
        print(f"Testing AIM Data Accuracy: {len(df)} rows")
        print("=" * 60)

    # Test results
    results = {"total": len(EXPECTED_VALUES), "passed": 0, "failed": 0, "failures": []}

    for i, test_case in enumerate(EXPECTED_VALUES):
        test_num = i + 1
        if verbose:
            print(
                f"\nTest {test_num}: Animal {test_case['animal_id']}, {test_case['session_date']}, {test_case['score_type']}"
            )

        # Find the matching row in CSV
        mask = (
            (df["sheet_name"] == test_case["sheet_name"])
            & (df["session_date"] == test_case["session_date"])
            & (df["animal_id"] == test_case["animal_id"])
            & (df["score_type"] == test_case["score_type"])
        )

        matching_rows = df[mask]

        if len(matching_rows) == 0:
            if verbose:
                print(f"  FAIL: No matching row found")
            results["failed"] += 1
            results["failures"].append({"test": test_num, "reason": "No matching row found", "expected": test_case})
            continue
        elif len(matching_rows) > 1:
            if verbose:
                print(f"  FAIL: Multiple matching rows found ({len(matching_rows)})")
            results["failed"] += 1
            results["failures"].append(
                {"test": test_num, "reason": f"Multiple matching rows ({len(matching_rows)})", "expected": test_case}
            )
            continue

        # Compare values
        csv_row = matching_rows.iloc[0]
        test_passed = True
        mismatches = []

        for time_point, expected_val in test_case["expected"].items():
            csv_val = csv_row[time_point]

            # Handle NaN comparisons
            if pd.isna(expected_val) and pd.isna(csv_val):
                continue
            elif pd.isna(expected_val) or pd.isna(csv_val):
                test_passed = False
                mismatches.append(f"{time_point}: expected {expected_val}, got {csv_val}")
            elif abs(float(expected_val) - float(csv_val)) > 0.001:
                test_passed = False
                mismatches.append(f"{time_point}: expected {expected_val}, got {csv_val}")

        if test_passed:
            if verbose:
                print(f"  PASS: All values match")
            results["passed"] += 1
        else:
            if verbose:
                print(f"  FAIL: Value mismatches - {'; '.join(mismatches)}")
            results["failed"] += 1
            results["failures"].append(
                {"test": test_num, "reason": "Value mismatches", "mismatches": mismatches, "expected": test_case}
            )

    return results


def load_genotype_data(data_connections_path: Path) -> Dict[str, Dict[str, str]]:
    """
    Load genotype data from the Data Connections CSV file.

    Parameters
    ----------
    data_connections_path : Path
        Path to the data_connections_D2_figures_3_4_6_7_8.csv file

    Returns
    -------
    Dict[str, Dict[str, str]]
        Dictionary mapping animal_id to genotype information
    """
    df = pd.read_csv(data_connections_path, header=4)
    genotype_map = {}

    for _, row in df.iterrows():
        if pd.notna(row["MOUSE"]) and str(row["MOUSE"]).strip():
            mouse_id = str(row["MOUSE"]).replace(",", "").strip()
            animal_type = row["ANIMAL"] if pd.notna(row["ANIMAL"]) else "unknown"
            category = row["Category"] if pd.notna(row["Category"]) else "unknown"

            # Skip non-numeric mouse IDs
            if mouse_id.isdigit():
                genotype_map[mouse_id] = {"animal_type": animal_type, "category": category}

    return genotype_map


def parse_session_date(cell_value: str) -> Optional[str]:
    """
    Parse session date from various formats found in the first column.

    Parameters
    ----------
    cell_value : str
        Cell value that might contain a session date

    Returns
    -------
    Optional[str]
        Parsed date in YYYY-MM-DD format, or None if not a valid session date
    """
    if not isinstance(cell_value, str):
        cell_value = str(cell_value)

    cell_value = cell_value.strip()

    # Pattern 1: MM/DD/YYYY (Nth session) format
    pattern1 = r"(\d{1,2})/(\d{1,2})/(\d{4})\s*\([^)]*session\)"
    match1 = re.search(pattern1, cell_value)
    if match1:
        month, day, year = match1.groups()
        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

    # Pattern 2: Date:MMDDYYYY format
    pattern2 = r"Date:(\d{2})(\d{2})(\d{4})"
    match2 = re.search(pattern2, cell_value)
    if match2:
        month, day, year = match2.groups()
        return f"{year}-{month}-{day}"

    # Pattern 3: Date:MM/DD(Nth session) format - infer year from sheet context
    pattern3 = r"Date:(\d{1,2})/(\d{1,2})\([^)]*session\)"
    match3 = re.search(pattern3, cell_value)
    if match3:
        month, day = match3.groups()
        # This pattern needs year context, will be handled in main function
        return f"INCOMPLETE-{month.zfill(2)}-{day.zfill(2)}"

    # Pattern 4: YYYY-MM-DD HH:MM:SS datetime format
    try:
        if len(cell_value) > 10 and ":" in cell_value:
            dt = datetime.fromisoformat(cell_value.replace(" 00:00:00", ""))
            return dt.strftime("%Y-%m-%d")
    except:
        pass

    return None


def is_session_marker(cell_value) -> bool:
    """Check if a cell value looks like a session date marker."""
    if pd.isna(cell_value):
        return False

    cell_str = str(cell_value).strip().lower()

    # Must contain session-related keywords and date-like patterns
    has_session_keyword = any(keyword in cell_str for keyword in ["session", "date:"])
    has_date_pattern = any(pattern in cell_str for pattern in ["/", "2017", "2018", "2019", "2020"])

    return has_session_keyword or has_date_pattern


def extract_genotype_from_text(text: str) -> Optional[str]:
    """Extract genotype from text string."""
    if not isinstance(text, str):
        text = str(text)

    text = text.strip().upper()

    if "(KO)" in text:
        return "KO"
    elif "(WT)" in text:
        return "WT"
    elif text == "(KO)":
        return "KO"
    elif text == "(WT)":
        return "WT"

    return None


def _parse_aim_excel_to_tidy_format(excel_path: Path, genotype_csv_path: Path, verbose: bool = False) -> pd.DataFrame:
    """
    Parse AIM Excel file to tidy wide format with genotype mapping.

    This function reads the Excel file and converts it to a tidy wide format where
    each row represents one animal-session-score_type combination.

    Parameters
    ----------
    excel_path : Path
        Path to the AIM Excel file
    genotype_csv_path : Path
        Path to the genotype mapping CSV file
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    pd.DataFrame
        Tidy wide format DataFrame with columns:
        - sheet_name: Original sheet name (e.g., "02232018")
        - session_date: Session date in YYYY-MM-DD format
        - session_number: Session number within the day (1, 2, 3, 4, 5)
        - animal_id: Animal identifier as string
        - genotype: Animal genotype (CDGI Knockout, Wild Type, or unknown)
        - score_type: Type of AIM score (axial, limb, orolingual)
        - 20min, 40min, 60min, 80min, 100min, 120min: Score values at each time point

    Examples
    --------
    Input Excel structure (one sheet):
    - Sheet name: "11202017"
    - Contains session dates and animal scoring data

    Output tidy format:
    | sheet_name | session_date | session_number | animal_id | genotype      | score_type | 20min | 40min | 60min | 80min | 100min | 120min |
    |------------|--------------|----------------|-----------|---------------|------------|--------|--------|--------|--------|---------|---------|
    | 11202017   | 2017-11-20   | 1              | 4124      | CDGI Knockout | axial      | 3.0    | 3.0    | 4.0    | 4.0    | 3.0     | NaN     |
    | 11202017   | 2017-11-20   | 1              | 4124      | CDGI Knockout | limb       | 1.5    | 2.5    | 2.0    | 2.0    | 1.0     | NaN     |
    | 11202017   | 2017-11-20   | 1              | 4124      | CDGI Knockout | orolingual | 2.0    | 2.0    | 2.0    | 1.5    | 3.0     | NaN     |
    """
    if verbose:
        print("Step 1: Parsing AIM Excel file to tidy format...")

    # Use the internal parser
    wide_df = parse_aim_excel_to_wide_format(excel_path, genotype_csv_path)

    if verbose:
        print(f"  Parsed {len(wide_df)} rows from Excel file")
        print(f"  Found {wide_df['animal_id'].nunique()} unique animals")
        print(f"  Found {wide_df['session_date'].nunique()} unique sessions")
        print(f"  Genotype distribution: {wide_df['genotype'].value_counts().to_dict()}")

    return wide_df


def _validate_tidy_data(wide_df: pd.DataFrame, verbose: bool = False) -> Tuple[bool, dict]:
    """
    Validate tidy data against test suite.

    This function runs the validation test suite against the parsed data to ensure
    accuracy. The test suite contains 23 hard-coded test cases extracted directly
    from the Excel files.

    Parameters
    ----------
    wide_df : pd.DataFrame
        Tidy wide format DataFrame to validate
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    Tuple[bool, dict]
        - bool: True if all tests passed, False otherwise
        - dict: Detailed test results with pass/fail counts and failure details

    Examples
    --------
    Test cases verify specific values like:
    - Animal 4124, session 2017-11-20, axial scores: 20min=3.0, 40min=3.0, 60min=4.0, etc.
    - Animal 4124, session 2017-11-22, orolingual scores: 20min=1.0, 40min=1.0, etc.

    Returns structure:
    {
        "passed": 23,
        "failed": 0,
        "total": 23,
        "failures": []  # List of failure details if any
    }
    """
    if verbose:
        print("Step 2: Validating parsed data against test suite...")

    # Run the test suite
    results = test_aim_data_accuracy(wide_df, verbose=verbose)

    if verbose:
        print(f"  Test results: {results['passed']}/{results['total']} passed")

    # Always print test failures regardless of verbose mode
    if results["failed"] > 0:
        print(f"  {results['failed']} tests failed")
        for failure in results["failures"]:
            print(f"    - {failure['reason']}")

    return results["failed"] == 0, results


def _transform_tidy_to_long_format(wide_df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:
    """
    Transform tidy wide format to long format for NWB optimization.

    This step converts the wide format (one row per animal-session-score_type) to
    long format (one row per animal-session-time_point) which is optimal for the
    NWB DynamicTable structure and enables trivial Figure 7J reproduction.

    Parameters
    ----------
    wide_df : pd.DataFrame
        Tidy wide format DataFrame
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    pd.DataFrame
        Long format DataFrame with columns:
        - animal_id: Animal identifier
        - session_date: Session date in YYYY-MM-DD format
        - session_number: Session number within the day (1, 2, 3, 4, 5)
        - genotype: Animal genotype
        - time_minutes: Time post L-DOPA in minutes
        - score_type: Type of AIM score (axial, limb, orolingual)
        - score_value: Score value at that time point

    Examples
    --------
    Input wide format:
    | animal_id | session_date | session_number | genotype      | score_type | 20min | 40min | 60min |
    |-----------|--------------|----------------|---------------|------------|--------|--------|--------|
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | axial      | 3.0    | 3.0    | 4.0    |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | limb       | 1.5    | 2.5    | 2.0    |

    Output long format:
    | animal_id | session_date | session_number | genotype      | time_minutes | score_type | score_value |
    |-----------|--------------|----------------|---------------|--------------|------------|-------------|
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | axial      | 3.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | axial      | 3.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 60           | axial      | 4.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | limb       | 1.5         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | limb       | 2.5         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 60           | limb       | 2.0         |

    Why this transformation is necessary:
    - Enables simple groupby operations for Figure 7J
    - Optimal for NWB DynamicTable structure
    - Facilitates mean/SEM calculations across animals
    - Simplifies data analysis workflows
    """
    if verbose:
        print("Step 3: Transforming wide format to long format...")

    # Define time point columns
    time_columns = ["20min", "40min", "60min", "80min", "100min", "120min"]

    # Identify columns to keep as identifiers
    id_columns = ["animal_id", "session_date", "session_number", "genotype", "score_type"]

    # Melt the DataFrame to long format
    long_df = pd.melt(
        wide_df, id_vars=id_columns, value_vars=time_columns, var_name="time_point", value_name="score_value"
    )

    # Convert time_point to minutes (remove "min" suffix)
    long_df["time_minutes"] = long_df["time_point"].str.replace("min", "").astype(int)

    # Drop the original time_point column
    long_df = long_df.drop("time_point", axis=1)

    # Remove rows with NaN scores (no data at that time point)
    long_df = long_df.dropna(subset=["score_value"])

    # Reorder columns
    long_df = long_df[
        ["animal_id", "session_date", "session_number", "genotype", "time_minutes", "score_type", "score_value"]
    ]

    # Sort by animal_id, session_date, session_number, time_minutes, score_type
    long_df = long_df.sort_values(["animal_id", "session_date", "session_number", "time_minutes", "score_type"])
    long_df = long_df.reset_index(drop=True)

    if verbose:
        print(f"  Transformed to {len(long_df)} rows in long format")
        print(f"  Time points: {sorted(long_df['time_minutes'].unique())}")
        print(f"  Score types: {sorted(long_df['score_type'].unique())}")

    return long_df


def _pivot_long_to_final_structure(long_df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:
    """
    Pivot long format to final structure for NWB DynamicTable.

    This final step pivots the long format data to create the optimized structure
    where each row represents one time point for one animal, with all score types
    (axial, limb, orolingual) in separate columns. This structure is perfect for
    the NWB DynamicTable and enables trivial Figure 7J reproduction.

    Parameters
    ----------
    long_df : pd.DataFrame
        Long format DataFrame
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    pd.DataFrame
        Final pivot DataFrame with columns:
        - animal_id: Animal identifier
        - session_date: Session date in YYYY-MM-DD format
        - session_number: Session number within the day (1, 2, 3, 4, 5)
        - genotype: Animal genotype
        - time_minutes: Time post L-DOPA in minutes
        - timestamps: Time in seconds (for NWB timestamps)
        - axial: Axial dyskinesia score
        - limb: Limb dyskinesia score
        - orolingual: Orolingual dyskinesia score
        - total_score: Total AIM score (sum of components)

    Examples
    --------
    Input long format:
    | animal_id | session_date | session_number | genotype      | time_minutes | score_type | score_value |
    |-----------|--------------|----------------|---------------|--------------|------------|-------------|
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | axial      | 3.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | limb       | 1.5         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | orolingual | 2.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | axial      | 3.0         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | limb       | 2.5         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | orolingual | 2.0         |

    Output final structure:
    | animal_id | session_date | session_number | genotype      | time_minutes | timestamps | axial | limb | orolingual | total_score |
    |-----------|--------------|----------------|---------------|--------------|------------|-------|------|------------|-------------|
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 20           | 1200.0     | 3.0   | 1.5  | 2.0        | 6.5         |
    | 4124      | 2017-11-20   | 1              | CDGI Knockout | 40           | 2400.0     | 3.0   | 2.5  | 2.0        | 7.5         |

    Why this structure is optimal:
    - Each row = one time point for one animal
    - All score types in same row (no joins needed)
    - Enables one-line data extraction: table.to_dataframe()
    - Perfect for groupby operations: df.groupby(['genotype', 'time_minutes']).mean()
    - Direct path from NWB to Figure 7J reproduction
    """
    if verbose:
        print("Step 4: Pivoting to final structure for NWB DynamicTable...")

    # Pivot to get axial, limb, orolingual in separate columns
    pivot_df = long_df.pivot_table(
        index=["animal_id", "session_date", "session_number", "genotype", "time_minutes"],
        columns="score_type",
        values="score_value",
        aggfunc="first",
    ).reset_index()

    # Fill missing score types with NaN
    for score_type in ["axial", "limb", "orolingual"]:
        if score_type not in pivot_df.columns:
            pivot_df[score_type] = np.nan

    # Calculate total score (sum of components, requires at least one non-NaN value)
    pivot_df["total_score"] = pivot_df[["axial", "limb", "orolingual"]].sum(axis=1, min_count=1)

    # Convert time to timestamps (time in seconds for NWB)
    pivot_df["timestamps"] = pivot_df["time_minutes"] * 60.0

    # Reorder columns for clarity
    column_order = [
        "animal_id",
        "session_date",
        "session_number",
        "genotype",
        "time_minutes",
        "timestamps",
        "axial",
        "limb",
        "orolingual",
        "total_score",
    ]
    pivot_df = pivot_df[column_order]

    # Sort by animal_id, session_date, session_number, time_minutes
    pivot_df = pivot_df.sort_values(["animal_id", "session_date", "session_number", "time_minutes"])
    pivot_df = pivot_df.reset_index(drop=True)

    if verbose:
        print(f"  Final structure: {len(pivot_df)} rows")
        print(f"  Columns: {list(pivot_df.columns)}")
        print(f"  Time points: {sorted(pivot_df['time_minutes'].unique())}")
        print(f"  Animals: {pivot_df['animal_id'].nunique()}")

    return pivot_df


# =============================================================================
# Figure 8 Specific Validation Tests
# =============================================================================


def test_aim_data_accuracy_figure8(wide_df: pd.DataFrame, verbose: bool = False) -> Dict[str, Any]:
    """
    Test suite for Figure 8 AIM data accuracy.

    This function validates that the parsed Figure 8 data matches expected values
    from manual inspection of the Excel file.

    Parameters
    ----------
    wide_df : pd.DataFrame
        Parsed wide format DataFrame from Figure 8
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    Dict[str, Any]
        Test results with pass/fail counts and failure details
    """
    if verbose:
        print("Testing Figure 8 AIM Data Accuracy:", len(wide_df), "rows")
        print("=" * 60)

    failures = []
    test_count = 0

    # Define test cases based on manual inspection of Figure 8 Excel
    test_cases = [
        # Session 1 (2023-09-27) - First date
        {
            "animal_id": 5844,
            "session_date": "2023-09-27",
            "score_type": "axial",
            "20min": 4.0,
            "40min": 4.0,
            "60min": 4.0,
            "80min": 4.0,
            "100min": 1.0,
        },
        {
            "animal_id": 5844,
            "session_date": "2023-09-27",
            "score_type": "limb",
            "20min": 2.5,
            "40min": 2.5,
            "60min": 2.5,
            "80min": 2.0,
            "100min": 1.0,
        },
        {
            "animal_id": 5844,
            "session_date": "2023-09-27",
            "score_type": "orolingual",
            "20min": 2.0,
            "40min": 2.0,
            "60min": 2.0,
            "80min": 2.0,
            "100min": 1.0,
        },
        {
            "animal_id": 5845,
            "session_date": "2023-09-27",
            "score_type": "axial",
            "20min": 4.0,
            "40min": 4.0,
            "60min": 4.0,
            "80min": 3.0,
            "100min": 1.0,
        },
        {
            "animal_id": 5846,
            "session_date": "2023-09-27",
            "score_type": "orolingual",
            "20min": 2.0,
            "40min": 2.0,
            "60min": 2.0,
            "80min": 1.5,
            "100min": 1.0,
        },
        # Session 2 (2023-09-29) - Second date
        {
            "animal_id": 5844,
            "session_date": "2023-09-29",
            "score_type": "axial",
            "20min": 4.0,
            "40min": 4.0,
            "60min": 4.0,
            "80min": 4.0,
            "100min": 3.0,
        },
        {
            "animal_id": 5846,
            "session_date": "2023-09-29",
            "score_type": "axial",
            "20min": 3.0,
            "40min": 4.0,
            "60min": 4.0,
            "80min": 3.5,
            "100min": 2.0,
        },
    ]

    for test_case in test_cases:
        test_count += 1
        animal_id = test_case["animal_id"]
        session_date = test_case["session_date"]
        score_type = test_case["score_type"]

        if verbose:
            print(f"\nTest {test_count}: Animal {animal_id}, {session_date}, {score_type}")

        # Find matching row
        mask = (
            (wide_df["animal_id"] == animal_id)
            & (wide_df["session_date"] == session_date)
            & (wide_df["score_type"] == score_type)
        )
        matching_rows = wide_df[mask]

        if len(matching_rows) == 0:
            failure_msg = f"No data found for animal {animal_id}, {session_date}, {score_type}"
            failures.append({"test": test_count, "reason": failure_msg})
            if verbose:
                print(f"  FAIL: {failure_msg}")
            continue
        elif len(matching_rows) > 1:
            failure_msg = f"Multiple rows found for animal {animal_id}, {session_date}, {score_type}"
            failures.append({"test": test_count, "reason": failure_msg})
            if verbose:
                print(f"  FAIL: {failure_msg}")
            continue

        row = matching_rows.iloc[0]

        # Check each time point
        time_points = ["20min", "40min", "60min", "80min", "100min", "120min"]
        all_match = True
        mismatches = []

        for time_point in time_points:
            if time_point in test_case:
                expected = test_case[time_point]
                actual = row[time_point]

                # Handle NaN comparisons
                if pd.isna(expected) and pd.isna(actual):
                    continue
                elif pd.isna(expected) or pd.isna(actual):
                    all_match = False
                    mismatches.append(f"{time_point}: expected {expected}, got {actual}")
                elif abs(expected - actual) > 0.001:  # Small tolerance for floating point
                    all_match = False
                    mismatches.append(f"{time_point}: expected {expected}, got {actual}")

        if all_match:
            if verbose:
                print("  PASS: All values match")
        else:
            failure_msg = f"Animal {animal_id}, {session_date}, {score_type}: {'; '.join(mismatches)}"
            failures.append({"test": test_count, "reason": failure_msg})
            if verbose:
                print(f"  FAIL: {failure_msg}")

    passed = test_count - len(failures)

    if verbose:
        print(f"\nTest Summary: {passed}/{test_count} passed")

    return {"passed": passed, "failed": len(failures), "total": test_count, "failures": failures}


# =============================================================================
# Figure 8 Specific Parsing Functions
# =============================================================================


def parse_aim_excel_to_wide_format_figure8(excel_path: Path) -> pd.DataFrame:
    """
    Parse Figure 8 M1R CRISPR AIM Excel file to wide format.

    Figure 8 has a different format where animal IDs include genotype information
    like "ET#5844 (control)" and "ET#5845 (M1R CRISPR)".

    Parameters
    ----------
    excel_path : Path
        Path to the Figure 8 AIM Excel file

    Returns
    -------
    pd.DataFrame
        Wide format DataFrame with extracted data
    """
    # Read the Excel file
    df = pd.read_excel(excel_path, sheet_name=0)

    results = []
    current_session_date = None
    current_animal_id = None
    current_genotype = None
    current_weight = None

    # Default to first session if no date found initially
    if current_session_date is None:
        current_session_date = "2023-09-27"  # First session date

    for idx, row in df.iterrows():
        # Extract session date from first column (like "Date: 09272023(1st)" or "Date:09292023 (2nd)")
        if pd.notna(row.iloc[0]) and "Date" in str(row.iloc[0]):
            date_str = str(row.iloc[0])
            # Try multiple date formats
            date_match = re.search(r"(\d{8})", date_str)
            if date_match:
                date_str = date_match.group(1)
                # Convert MMDDYYYY to YYYY-MM-DD
                month = date_str[:2]
                day = date_str[2:4]
                year = date_str[4:]
                current_session_date = f"{year}-{month}-{day}"
            # Also handle MM/DD/YYYY format
            elif re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", date_str):
                date_match = re.search(r"(\d{1,2})/(\d{1,2})/(\d{4})", date_str)
                month = date_match.group(1).zfill(2)
                day = date_match.group(2).zfill(2)
                year = date_match.group(3)
                current_session_date = f"{year}-{month}-{day}"
            continue

        # Extract animal ID and genotype (like "ET#5844 (control)" or just "ET#5844")
        if pd.notna(row.iloc[0]) and str(row.iloc[0]).startswith("ET#"):
            animal_line = str(row.iloc[0])
            # Try with genotype in parentheses first
            match = re.match(r"ET#(\d+)\s*\(([^)]+)\)", animal_line)
            if match:
                current_animal_id = int(match.group(1))
                genotype_raw = match.group(2).strip()
                # Standardize genotype names
                if "control" in genotype_raw.lower():
                    current_genotype = "Control"
                elif "crispr" in genotype_raw.lower():
                    current_genotype = "M1R CRISPR"
                else:
                    current_genotype = genotype_raw
            else:
                # Try without genotype (like "ET#5844")
                match = re.match(r"ET#(\d+)", animal_line)
                if match:
                    current_animal_id = int(match.group(1))
                    # Infer genotype based on animal ID (from first occurrence)
                    if current_animal_id == 5844:
                        current_genotype = "Control"
                    elif current_animal_id in [5845, 5846]:
                        current_genotype = "M1R CRISPR"
                    else:
                        current_genotype = "Unknown"
            # Don't continue - check if this row also has score data

        # Extract weight (like "    g" in next row)
        if pd.notna(row.iloc[0]) and "g" in str(row.iloc[0]) and current_animal_id:
            current_weight = str(row.iloc[0]).strip()
            # Don't continue - this row might also have score data

        # Extract score data
        if (
            pd.notna(row.iloc[1])
            and str(row.iloc[1]) in ["Ax", "Li", "Ol"]
            and current_session_date
            and current_animal_id
            and current_genotype
        ):

            score_type = str(row.iloc[1])
            score_type_name = {"Ax": "axial", "Li": "limb", "Ol": "orolingual"}[score_type]

            # Extract time point scores (columns 2-7 typically contain 20min, 40min, etc.)
            time_points = ["20min", "40min", "60min", "80min", "100min", "120min"]
            scores = {}

            # Start from column 2 (index 2) and extract scores
            for i, time_point in enumerate(time_points):
                col_idx = 2 + i
                if col_idx < len(row):
                    score_val = row.iloc[col_idx]
                    if pd.notna(score_val) and score_val != "":
                        try:
                            scores[time_point] = float(score_val)
                        except (ValueError, TypeError):
                            scores[time_point] = np.nan
                    else:
                        scores[time_point] = np.nan
                else:
                    scores[time_point] = np.nan

            # Create record
            record = {
                "animal_id": current_animal_id,
                "session_date": current_session_date,
                "genotype": current_genotype,
                "score_type": score_type_name,
                "weight": current_weight,
                **scores,
            }
            results.append(record)

    if not results:
        raise ValueError("No AIM data found in Excel file")

    wide_df = pd.DataFrame(results)

    # Add session numbers (assuming chronological order within each animal)
    wide_df = wide_df.sort_values(["animal_id", "session_date", "score_type"])
    wide_df["session_number"] = wide_df.groupby("animal_id").cumcount() // 3 + 1  # 3 score types per session

    return wide_df


def _parse_aim_excel_to_tidy_format_figure8(excel_path: Path, verbose: bool = False) -> pd.DataFrame:
    """
    Parse Figure 8 AIM Excel file to tidy format without genotype CSV dependency.

    Parameters
    ----------
    excel_path : Path
        Path to the Figure 8 AIM Excel file
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    pd.DataFrame
        Tidy format DataFrame
    """
    if verbose:
        print("Step 1: Parsing Figure 8 AIM Excel file to tidy format...")

    # Parse the Excel file
    wide_df = parse_aim_excel_to_wide_format_figure8(excel_path)

    if verbose:
        print(f"  Parsed {len(wide_df)} rows from Excel")
        print(f"  Animals: {sorted(wide_df['animal_id'].unique())}")
        print(f"  Genotypes: {sorted(wide_df['genotype'].unique())}")
        print(f"  Session dates: {sorted(wide_df['session_date'].unique())}")

    return wide_df


def build_source_data_from_aim_excel_table_figure8(excel_path: Path, verbose: bool = False) -> pd.DataFrame:
    """
    Build source data from Figure 8 AIM Excel table for NWB conversion.

    This is the main entry point for Figure 8 data processing, providing a complete
    pipeline from raw Excel to NWB-ready format with validation tests.

    Parameters
    ----------
    excel_path : Path
        Path to the Figure 8 AIM Excel file
    verbose : bool, default=False
        Enable verbose output

    Returns
    -------
    pd.DataFrame
        Final pivot DataFrame ready for NWB conversion
    """
    if verbose:
        print("Building source data from Figure 8 AIM Excel table...")
        print("=" * 60)

    # Step 1: Parse Excel to tidy format
    wide_df = _parse_aim_excel_to_tidy_format_figure8(excel_path, verbose)

    # Step 2: Validate parsed data against test suite
    if verbose:
        print("Step 2: Validating parsed data against Figure 8 test suite...")

    # Run the validation tests
    results = test_aim_data_accuracy_figure8(wide_df, verbose=verbose)

    if verbose:
        print(f"  Test results: {results['passed']}/{results['total']} passed")

    # Always print test failures regardless of verbose mode
    if results["failed"] > 0:
        print(f"  {results['failed']} tests failed")
        for failure in results["failures"]:
            print(f"    - {failure['reason']}")

    # Step 3: Transform to long format
    long_df = _transform_tidy_to_long_format(wide_df, verbose)

    # Step 4: Pivot to final structure
    pivot_df = _pivot_long_to_final_structure(long_df, verbose)

    if verbose:
        print("\nFigure 8 data processing completed successfully!")
        print(f"Final dataset: {len(pivot_df)} rows, {len(pivot_df.columns)} columns")
        print(f"Animals: {pivot_df['animal_id'].nunique()}")
        print(f"Sessions: {pivot_df.groupby(['animal_id', 'session_date']).ngroups}")
        print(f"Time points: {sorted(pivot_df['time_minutes'].unique())}")

    return pivot_df
